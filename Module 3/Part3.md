#### Gradient Descent

---

* batch gradient descent
* stochastic gradient descent(m = 1)

* momentum
* AdaGrad / RMSProp
* Adam (RMSProp + Momentum)

* regularization to avoid overfitting