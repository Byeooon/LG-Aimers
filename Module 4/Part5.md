#### Transformer

---

* self-attention
* mutil-head attention

* layer normalization

* query, key, value

* hidden state vectorn
* position encoding

* masked self-attention

